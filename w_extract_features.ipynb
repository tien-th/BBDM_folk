{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/BBDM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.BrownianBridge.LatentBrownianBridgeModel import LatentBrownianBridgeModel \n",
    "from model.VQGAN.vqgan import VQModel \n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "# from model.reg.reg import Reg, Transformer_2D\n",
    "\n",
    "import torch \n",
    "# checkpoint = '/mnt/disk3/tiennh/taming-transformers/logs/2023-09-14T22-05-12_custom_vqgan/checkpoints/last.ckpt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from runners.base.EMA import EMA\n",
    "# w_extract_features.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "Restored from /home/PET-CT/vqgan/vq1_3_69.ckpt\n",
      "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
      "Restored from /home/PET-CT/vqgan/vq1_3_69.ckpt\n",
      "load vqgan from /home/PET-CT/vqgan/vq1_3_69.ckpt\n",
      "Spatial Rescaler mapping from 1 to 3 channels after resizing.\n",
      "Spatial Rescaler mapping from 1 to 3 channels after resizing.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import argparse \n",
    "import omegaconf.dictconfig\n",
    "import os\n",
    "f = open('configs/Template_CPDM.yaml', 'r')\n",
    "dict_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "IMAGE_SIZE = (256,256)\n",
    "MAX_PIXEL = 2047\n",
    "\n",
    "transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.0),\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "def dict2namespace(config):\n",
    "    namespace = argparse.Namespace()\n",
    "    for key, value in config.items():\n",
    "        if isinstance(value, dict) or isinstance(value, omegaconf.dictconfig.DictConfig):\n",
    "            new_value = dict2namespace(value)\n",
    "        else:\n",
    "            new_value = value\n",
    "        setattr(namespace, key, new_value)\n",
    "    return namespace\n",
    "\n",
    "nconfig = dict2namespace(dict_config)\n",
    "nconfig.model.model_load_path = 'results1/229_conditional_LBBDM_segment_atten_2encoders/LBBDM-f4/checkpoint/top_model_epoch_48.pth'\n",
    "nconfig.training.device = 'cuda:2'\n",
    "\n",
    "vq = VQModel(**vars(nconfig.model.VQGAN.params))\n",
    "model = LatentBrownianBridgeModel(nconfig.model)\n",
    "\n",
    "\n",
    "# model.vqgan = vq.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model LBBDM-f4 from results1/229_conditional_LBBDM_segment_atten_2encoders/LBBDM-f4/checkpoint/top_model_epoch_48.pth\n"
     ]
    }
   ],
   "source": [
    "def load_model_from_checkpoint(config, net):\n",
    "    model_states = None\n",
    "    # if config.model.__contains__('model_load_path') and config.model.model_load_path is not None:\n",
    "    print(f\"load model {config.model.model_name} from {config.model.model_load_path}\")\n",
    "    model_states = torch.load(config.model.model_load_path, map_location='cpu')\n",
    "\n",
    "    global_epoch = model_states['epoch']\n",
    "    global_step = model_states['step']\n",
    "\n",
    "    # load model\n",
    "    \n",
    "    ema = EMA(config.model.EMA.ema_decay)\n",
    "    update_ema_interval = config.model.EMA.update_ema_interval\n",
    "    start_ema_step = config.model.EMA.start_ema_step\n",
    "    ema.register(net)\n",
    "\n",
    "    net.load_state_dict(model_states['model'])\n",
    "    \n",
    "   \n",
    "    # Load EMA\n",
    "    if config.model.EMA.use_ema:\n",
    "        ema.shadow = model_states['ema']\n",
    "        ema.reset_device(net)\n",
    "        ema.apply_shadow(net)\n",
    "        \n",
    "    net = net.to(config.training.device)\n",
    "    return net, ema \n",
    "\n",
    "model, ema = load_model_from_checkpoint(nconfig, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = model.denoise_fn\n",
    "# unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_to_eval( net, y, y_name, config, callback_save_features=None):\n",
    "    to_normal = config.data.dataset_config.to_normal\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y = y.to(config.training.device)\n",
    "        net = net.to(config.training.device)\n",
    "        out_samples, one_step_samples, _ = net.sample(y, [y_name], 'val_step', clip_denoised=False ,sample_mid_step=True, callback=callback_save_features)\n",
    "            \n",
    "        return out_samples, one_step_samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np  \n",
    "\n",
    "index = 14211\n",
    "x_np = np.load(f'/home/PET-CT/splited_data_15k/test/A/{index}.npy', allow_pickle=True)\n",
    "x_np = x_np / float(MAX_PIXEL)\n",
    "image = Image.fromarray(x_np)\n",
    "x = transform(image)\n",
    "x = x.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_for_save_features = 'features/CPDM'\n",
    "feature_maps_path = os.path.join(root_dir_for_save_features, str(index))\n",
    "os.makedirs(feature_maps_path, exist_ok=True)\n",
    "\n",
    "def save_feature_map(feature_map, dirname , filename):\n",
    "        save_path = os.path.join(dirname, f\"{filename}.pt\")\n",
    "        torch.save(feature_map, save_path)\n",
    "\n",
    "\n",
    "def callback_save_features(img, pred_x0, i): \n",
    "    dir_name = os.path.join(feature_maps_path, 'input_blocks')\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    save_feature_maps(unet_model.input_blocks, dir_name, i)\n",
    "    dir_name = os.path.join(feature_maps_path, 'output_blocks')\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    save_feature_maps(unet_model.output_blocks, dir_name, i)\n",
    "\n",
    "def save_feature_maps(blocks, dir_name, i):\n",
    "    block_idx = 0\n",
    "    for block in blocks:\n",
    "        if block_idx < 3:\n",
    "            block_idx += 1\n",
    "            continue\n",
    "        if \"ResBlock\" in str(type(block[0])):\n",
    "            in_layers_features = block[0].in_layers_features\n",
    "            out_layers_features = block[0].out_layers_features\n",
    "            if in_layers_features is not None:\n",
    "                save_feature_map(in_layers_features, dir_name, f\"block_{block_idx}_in_layers_features_time_{i}\")\n",
    "            else : \n",
    "                print(f\"block {block_idx} timestep {i} has in_layers_features: None \")\n",
    "                \n",
    "            if out_layers_features is not None:\n",
    "                save_feature_map(out_layers_features, dir_name, f\"block_{block_idx}_out_layers_features_time_{i}\")\n",
    "            else :\n",
    "                print(f\"block {block_idx} timestep {i} has out_layers_features: None \")\n",
    "        block_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 200/200 [00:15<00:00, 13.32it/s]\n",
      "save output sample mid steps: 100%|██████████| 201/201 [00:05<00:00, 38.31it/s]\n",
      "save one step sample mid steps: 100%|██████████| 200/200 [00:05<00:00, 38.85it/s]\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "model.eval()\n",
    "\n",
    "out_samples, one_step_samples = sample_to_eval(model, x, index, nconfig, callback_save_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_step_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384, 16, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_model.output_blocks[4][0].in_layers_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unet_model.output_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "midsteps = torch.arange(1000 - 1, 1, step=-((1000 - 1) / (200 - 2))).long()\n",
    "steps = torch.cat((midsteps, torch.Tensor([1, 0]).long()), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([999, 993, 988, 983, 978, 973, 968, 963, 958, 953, 948, 943, 938, 933,\n",
       "        928, 923, 918, 913, 908, 903, 898, 893, 888, 882, 877, 872, 867, 862,\n",
       "        857, 852, 847, 842, 837, 832, 827, 822, 817, 812, 807, 802, 797, 792,\n",
       "        787, 782, 777, 771, 766, 761, 756, 751, 746, 741, 736, 731, 726, 721,\n",
       "        716, 711, 706, 701, 696, 691, 686, 681, 676, 671, 666, 660, 655, 650,\n",
       "        645, 640, 635, 630, 625, 620, 615, 610, 605, 600, 595, 590, 585, 580,\n",
       "        575, 570, 565, 560, 555, 549, 544, 539, 534, 529, 524, 519, 514, 509,\n",
       "        504, 499, 494, 489, 484, 479, 474, 469, 464, 459, 454, 449, 444, 438,\n",
       "        433, 428, 423, 418, 413, 408, 403, 398, 393, 388, 383, 378, 373, 368,\n",
       "        363, 358, 353, 348, 343, 338, 333, 327, 322, 317, 312, 307, 302, 297,\n",
       "        292, 287, 282, 277, 272, 267, 262, 257, 252, 247, 242, 237, 232, 227,\n",
       "        222, 216, 211, 206, 201, 196, 191, 186, 181, 176, 171, 166, 161, 156,\n",
       "        151, 146, 141, 136, 131, 126, 121, 116, 111, 105, 100,  95,  90,  85,\n",
       "         80,  75,  70,  65,  60,  55,  50,  45,  40,  35,  30,  25,  20,  15,\n",
       "         10,   5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([999, 993, 988, 983, 978, 973, 968, 963, 958, 953, 948, 943, 938, 933,\n",
       "        928, 923, 918, 913, 908, 903, 898, 893, 888, 882, 877, 872, 867, 862,\n",
       "        857, 852, 847, 842, 837, 832, 827, 822, 817, 812, 807, 802, 797, 792,\n",
       "        787, 782, 777, 771, 766, 761, 756, 751, 746, 741, 736, 731, 726, 721,\n",
       "        716, 711, 706, 701, 696, 691, 686, 681, 676, 671, 666, 660, 655, 650,\n",
       "        645, 640, 635, 630, 625, 620, 615, 610, 605, 600, 595, 590, 585, 580,\n",
       "        575, 570, 565, 560, 555, 549, 544, 539, 534, 529, 524, 519, 514, 509,\n",
       "        504, 499, 494, 489, 484, 479, 474, 469, 464, 459, 454, 449, 444, 438,\n",
       "        433, 428, 423, 418, 413, 408, 403, 398, 393, 388, 383, 378, 373, 368,\n",
       "        363, 358, 353, 348, 343, 338, 333, 327, 322, 317, 312, 307, 302, 297,\n",
       "        292, 287, 282, 277, 272, 267, 262, 257, 252, 247, 242, 237, 232, 227,\n",
       "        222, 216, 211, 206, 201, 196, 191, 186, 181, 176, 171, 166, 161, 156,\n",
       "        151, 146, 141, 136, 131, 126, 121, 116, 111, 105, 100,  95,  90,  85,\n",
       "         80,  75,  70,  65,  60,  55,  50,  45,  40,  35,  30,  25,  20,  15,\n",
       "         10,   5,   1,   0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m----> 2\u001b[0m int1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "int = '10' \n",
    "int1 = int(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BBDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
