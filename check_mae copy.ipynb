{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MAE: 701.488388136485\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "# Define the function to compute MAE\n",
    "def compute_mae(image1, image2):\n",
    "    return np.abs(image1 - image2).mean()\n",
    "\n",
    "def compute_mape(image1, image2):\n",
    "    return np.abs((image1 - image2) / (image1 + 1e-6)).mean() * 100\n",
    "\n",
    "# Paths to the folders\n",
    "gt_folder = \"/home/PET-CT/splited_data_15k/test/B\"\n",
    "condition_folder = \"/home/PET-CT/splited_data_15k/test/A\"\n",
    "# pre_folder = \"results/108_CT2PET_UncerBBDM3c/LBBDM-f4/sample_to_eval/200\"\n",
    "\n",
    "dataset_name = '1218_segmented_conditional_LBBDM'\n",
    "\n",
    "pre_folder = \"/home/PET-CT/thaind/BBDM_folk/results/\" + dataset_name + \"/LBBDM-f4/sample_to_eval/200\"\n",
    "confidence_folder = \"/home/PET-CT/thaind/BBDM_folk/results/\" + dataset_name + \"/LBBDM-f4/sample_to_eval/additional_condition\"\n",
    "# confidence_folder2 = \"/home/PET-CT/thaind/BBDM_folk/results/\" + dataset_name + \"/LBBDM-f4/sample_to_eval/confidence_2\"\n",
    "\n",
    "\n",
    "# mode = \"low_mape\"\n",
    "mode = \"high_mape\"\n",
    "\n",
    "confidences, confidences2 = [], []\n",
    "mae_scores = []\n",
    "high_mae, high_mae_gts, high_mae_pds, high_mae_conditions = [], [], [], []\n",
    "\n",
    "# Iterate through the files in the ground truth folder\n",
    "for filename in os.listdir(gt_folder):\n",
    "    # Make sure the file is a numpy array\n",
    "    if filename.endswith(\".npy\"):\n",
    "        # Construct the paths for the corresponding ground truth and predicted files\n",
    "        try:\n",
    "            gt_path = os.path.join(gt_folder, filename)\n",
    "            pre_path = os.path.join(pre_folder, filename)\n",
    "            confidence_path = os.path.join(confidence_folder, filename)\n",
    "            # confidence_path2 = os.path.join(confidence_folder2, filename)\n",
    "\n",
    "            # Load the images as numpy arrays\n",
    "            gt_img = np.load(gt_path, allow_pickle=True)\n",
    "            pre_img = np.load(pre_path, allow_pickle=True)\n",
    "        except:\n",
    "            continue   \n",
    "        # Preprocess the predicted image\n",
    "        pre_img1 = pre_img.mean(axis=-1) / 32767.0\n",
    "        \n",
    "        # Normalize the ground truth image\n",
    "        gt_img1 = gt_img / 32767.0\n",
    "\n",
    "        # mae = compute_mae(pre_img1, gt_img1)\n",
    "        # mae_scores.append(mae * 32767)\n",
    "        mape = compute_mape(1-gt_img1, 1-pre_img1)\n",
    "        mae_scores.append(mape)\n",
    "\n",
    "        # if mae * 32767 < 110  : \n",
    "        if mode == \"low_mape\" and mape < 90 :\n",
    "            high_mae_gts.append(gt_img) \n",
    "            high_mae_pds.append(pre_img)\n",
    "            high_mae_conditions.append(np.load(os.path.join(condition_folder, filename), allow_pickle=True))\n",
    "            # high_mae.append(mae * 32767)\n",
    "            confidences.append(np.load(confidence_path, allow_pickle= True))\n",
    "            # confidences2.append(np.load(confidence_path2, allow_pickle= True))\n",
    "            high_mae.append(mape)\n",
    "        if mode == \"high_mape\" and mape > 1400: \n",
    "            high_mae_gts.append(gt_img) \n",
    "            high_mae_pds.append(pre_img)\n",
    "            high_mae_conditions.append(np.load(os.path.join(condition_folder, filename), allow_pickle=True))\n",
    "            # high_mae.append(mae * 32767)\n",
    "            confidences.append(np.load(confidence_path, allow_pickle= True))\n",
    "            # confidences2.append(np.load(confidence_path2, allow_pickle= True))\n",
    "            high_mae.append(mape)\n",
    "\n",
    "# Calculate the mean scores over all pairs\n",
    "# mean_ssim = np.mean(ssim_scores)\n",
    "# mean_psnr = np.mean(psnr_scores)\n",
    "mean_mae = np.mean(mae_scores)\n",
    "\n",
    "# Print the mean metrics\n",
    "# print(\"Mean SSIM: {}\".format(mean_ssim))\n",
    "# print(\"Mean PSNR: {}\".format(mean_psnr))\n",
    "print(\"Mean MAE: {}\".format(mean_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 18)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high_mae), len(high_mae_gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0061641211809418, 1527.8648453965313)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(mae_scores), np.max(mae_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def create_fol(dir_path): \n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "def visualize_high_mae_images(high_mae_gts, high_mae_pds, high_mae_conditions, confidence, save_folder):\n",
    "    for i in range(len(high_mae)):\n",
    "        mae = high_mae[i]\n",
    "\n",
    "        gt_img = high_mae_gts[i]\n",
    "        pre_img = high_mae_pds[i]\n",
    "        condition_img = high_mae_conditions[i]\n",
    "        confidence_img = confidences[i]\n",
    "        # confidence_img2 = confidences2[i]\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Display the ground truth image\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.imshow(gt_img, cmap='gray')\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Display the predicted image\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.imshow(pre_img, cmap='gray')\n",
    "        plt.title(\"Predicted\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Display the condition image\n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.imshow(condition_img, cmap='gray')\n",
    "        plt.title(\"Condition\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.imshow(confidence_img, cmap='gray')\n",
    "        plt.title(\"Additional_map\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # plt.subplot(1, 5, 5)\n",
    "        # plt.imshow(confidence_img2, cmap='gray')\n",
    "        # plt.title(\"confidence_Unet2\")\n",
    "        # plt.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"MAPE: {mae:.2f}\")\n",
    "\n",
    "        # Save the figure to the specified folder\n",
    "        save_path = os.path.join(save_folder, f\"mape_{mae:.2f}.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "# Specify the folder to save the images\n",
    "save_folder = \"./results/visualization/\" + mode + \"/\" + dataset_name \n",
    "create_fol(save_folder)\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# Visualize and save images with high MAE\n",
    "visualize_high_mae_images(high_mae_gts, high_mae_pds, high_mae_conditions, confidences, save_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_fol('./baddataset/train/A')\n",
    "create_fol('./baddataset/train/B')\n",
    "create_fol('./baddataset/val/A')\n",
    "create_fol('./baddataset/val/B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f16345c1fd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import shutil \n",
    "import os\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "# Define the function to compute MAE\n",
    "def compute_mae(image1, image2):\n",
    "    return np.abs(image1 - image2).mean()\n",
    "\n",
    "# Paths to the folders\n",
    "gt_folder = \"/home/PET-CT/splited_data_15k/train/B\"\n",
    "condition_folder = \"/home/PET-CT/splited_data_15k/train/A\"\n",
    "# pre_folder = \"results/108_CT2PET_UncerBBDM3c/LBBDM-f4/sample_to_eval/200\"\n",
    "\n",
    "dataset_name = 'UncerBBDM_1Unet_confloss_15k_training_samples_top_model'\n",
    "\n",
    "pre_folder = \"/home/PET-CT/thaind/BBDM_folk/results/\" + dataset_name + \"/LBBDM-f4/sample_to_eval/200\"\n",
    "\n",
    "mae_scores = []\n",
    "high_mae, high_mae_gts, high_mae_pds, high_mae_conditions = [], [], [], []\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through the files in the ground truth folder\n",
    "for filename in os.listdir(gt_folder):\n",
    "    # Make sure the file is a numpy array\n",
    "    if filename.endswith(\".npy\"):\n",
    "        # Construct the paths for the corresponding ground truth and predicted files\n",
    "        try:\n",
    "            gt_path = os.path.join(gt_folder, filename)\n",
    "            pre_path = os.path.join(pre_folder, filename)\n",
    "        \n",
    "            # Load the images as numpy arrays\n",
    "            gt_img = np.load(gt_path, allow_pickle=True)\n",
    "            pre_img = np.load(pre_path, allow_pickle=True)\n",
    "        except:\n",
    "            continue   \n",
    "        # Preprocess the predicted image\n",
    "        pre_img1 = pre_img.mean(axis=-1) / 32767.0\n",
    "        \n",
    "        # Normalize the ground truth image\n",
    "        gt_img1 = gt_img / 32767.0\n",
    "\n",
    "        mae = compute_mae(pre_img1, gt_img1)\n",
    "        mae_scores.append(mae * 32767)\n",
    "\n",
    "        if mae * 32767 > 1000  : \n",
    "            # shutil.copy(os.path.join(gt_folder, filename), os.path.join(bad_folder + '/B', filename) )\n",
    "            # shutil.copy(os.path.join(condition_folder, filename), os.path.join(bad_folder + '/A', filename) )\n",
    "            \n",
    "            high_mae_gts.append(gt_img) \n",
    "            high_mae_pds.append(pre_img)\n",
    "\n",
    "            high_mae_conditions.append(np.load(os.path.join(condition_folder, filename), allow_pickle=True))\n",
    "            high_mae.append(mae * 32767)\n",
    "\n",
    "# Calculate the mean scores over all pairs\n",
    "# mean_ssim = np.mean(ssim_scores)\n",
    "# mean_psnr = np.mean(psnr_scores)\n",
    "mean_mae = np.mean(mae_scores)\n",
    "\n",
    "# Print the mean metrics\n",
    "# print(\"Mean SSIM: {}\".format(mean_ssim))\n",
    "# print(\"Mean PSNR: {}\".format(mean_psnr))\n",
    "print(\"Mean MAE: {}\".format(mean_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int,long)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BBDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
